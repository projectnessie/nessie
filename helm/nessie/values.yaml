##
## Copyright (C) 2024 Dremio
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
## http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
##

# -- The number of replicas to deploy (horizontal scaling).
# Beware that replicas are stateless; don't set this number > 1 when using IN_MEMORY or ROCKSDB version store types.
replicaCount: 1

image:
  # -- The image repository to pull from.
  repository: ghcr.io/projectnessie/nessie
  # -- The image pull policy.
  pullPolicy: IfNotPresent
  # -- Overrides the image tag whose default is the chart version.
  tag: ""
  # -- The path to the directory where the application.properties file should be mounted.
  configDir: /deployments/config

# -- References to secrets in the same namespace to use for pulling any of the images used by this
# chart. Each entry is a LocalObjectReference to an existing secret in the namespace. The secret
# must contain a .dockerconfigjson key with a base64-encoded Docker configuration file. See
# https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ for more
# information.
imagePullSecrets: []
#  - name: registry-creds

# -- Logging configuration.
log:
  # -- The log level of the root category, which is used as the default log level for all categories.
  level: INFO
  # -- Configuration for the console appender.
  console:
    # -- Whether to enable the console appender.
    enabled: true
    # -- The log level of the console appender.
    threshold: ALL
    # -- Whether to log in JSON format.
    json: false
    # -- The log format to use. Ignored if JSON format is enabled. See
    # https://quarkus.io/guides/logging#logging-format for details.
    format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
  # -- Configuration for the file appender.
  file:
    # -- Whether to enable the file appender.
    enabled: false
    # -- The log level of the file appender.
    threshold: ALL
    # -- Whether to log in JSON format.
    json: false
    # -- The log format to use. Ignored if JSON format is enabled. See
    # https://quarkus.io/guides/logging#logging-format for details.
    format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %h %N[%i] %-5p [%X{traceId},%X{spanId},%X{sampled}] [%c{3.}] (%t) %s%e%n"
    # -- The local directory where log files are stored. The persistent volume claim will be mounted
    # here.
    logsDir: /deployments/logs
    # -- The log file name.
    fileName: nessie.log
    # -- Log rotation configuration.
    rotation:
      # -- The maximum size of the log file before it is rotated. Should be expressed as a Kubernetes quantity.
      maxFileSize: 100Mi
      # -- The maximum number of backup files to keep.
      maxBackupIndex: 5
      # -- An optional suffix to append to the rotated log files. If present, the rotated log files
      # will be grouped in time buckets, and each bucket will contain at most maxBackupIndex files.
      # The suffix must be in a date-time format that is understood by DateTimeFormatter. If the
      # suffix ends with .gz or .zip, the rotated files will also be compressed using the
      # corresponding algorithm.
      fileSuffix: ~  # .yyyy-MM-dd.gz
    # -- The log storage configuration. A persistent volume claim will be created using these
    # settings.
    storage:
      # -- The storage class name of the persistent volume claim to create.
      className: standard
      # -- The size of the persistent volume claim to create.
      size: 512Gi
      # -- Labels to add to the persistent volume claim spec selector; a persistent volume with
      # matching labels must exist. Leave empty if using dynamic provisioning.
      selectorLabels: {}
        # app.kubernetes.io/name: nessie
        # app.kubernetes.io/instance: RELEASE-NAME
  # -- Configuration for the Sentry appender. See https://sentry.io and
  # https://docs.quarkiverse.io/quarkus-logging-sentry/dev for more information.
  sentry:
    # -- Whether to enable the Sentry appender.
    enabled: false
    # -- The Sentry DSN. Required.
    dsn: ~  # "https://abcd@sentry.io/1234"
    # -- The log level of the Sentry appender.
    level: ERROR
    # -- The environment to report to Sentry. Optional.
    environment: ~
    # -- The release version to report to Sentry. Optional.
    release: ~
    # -- Package prefixes that belong to your application.
    inAppPackages:
    - org.projectnessie
  # -- Configuration for specific log categories.
  categories:
    org.projectnessie: INFO
    # Useful to debug configuration issues:
    # io.smallrye.config: DEBUG

# -- Which type of version store to use: IN_MEMORY, ROCKSDB, DYNAMODB2, MONGODB2, CASSANDRA2, JDBC2, BIGTABLE.
# Note: the version store type JDBC is deprecated, please use the Nessie Server Admin Tool to migrate to JDBC2.
# Note: the version store type CASSANDRA is deprecated, please use the Nessie Server Admin Tool to migrate to CASSANDRA2.
# Note: the version store type DYNAMODB is deprecated, please use the Nessie Server Admin Tool to migrate to DYNAMODB2.
# Note: the version store type MONGODB is deprecated, please use the Nessie Server Admin Tool to migrate to MONGODB2.
versionStoreType: IN_MEMORY

# Cassandra settings. Only required when using CASSANDRA version store type; ignored otherwise.
cassandra:
  keyspace: nessie
  # -- The contact points for the Cassandra cluster. At least one contact point must be provided,
  # but more can be added for redundancy. The format is a comma-separated list of host:port elements.
  contactPoints: cassandra.cassandra.svc.cluster.local:9042
  localDatacenter: datacenter1
  secret:
    # -- The secret name to pull Cassandra credentials from.
    name: cassandra-creds
    # -- The secret key storing the Cassandra username.
    username: cassandra_username
    # -- The secret key storing the Cassandra password.
    password: cassandra_password

# RocksDB settings. Only required when using ROCKSDB version store type; ignored otherwise.
rocksdb:
  # -- The storage class name of the persistent volume claim to create.
  storageClassName: standard
  # -- The size of the persistent volume claim to create.
  storageSize: 1Gi
  # -- Labels to add to the persistent volume claim spec selector; a persistent volume with matching labels must exist.
  # Leave empty if using dynamic provisioning.
  selectorLabels:
    {}
    # app.kubernetes.io/name: nessie
    # app.kubernetes.io/instance: RELEASE-NAME

# DynamoDB settings. Only required when using DYNAMODB version store type; ignored otherwise.
dynamodb:
  # -- The AWS region to use.
  region: us-west-2
  # -- The name of the profile that should be used, when loading AWS credentials from a profile
  # file. Required only if no secret is provided below.
  profile: default
  secret:
    # -- The secret name to pull AWS credentials from. Optional; if not present, the default AWS
    # credentials provider chain is used.
    name: awscreds
    # -- The secret key storing the AWS secret key id.
    awsAccessKeyId: aws_access_key_id
    # -- The secret key storing the AWS secret access key.
    awsSecretAccessKey: aws_secret_access_key

## Mongo DB settings. Only required when using MONGODB version store type; ignored otherwise.
mongodb:
  # -- The MongoDB database name.
  name: nessie
  # -- The MongoDB connection string.
  connectionString: mongodb://localhost:27017
  secret:
    # -- The secret name to pull MongoDB credentials from.
    name: mongodb-creds
    # -- The secret key storing the MongoDB username.
    username: mongodb_username
    # -- The secret key storing the MongoDB password.
    password: mongodb_password

# JDBC datasource settings. Only required when using JDBC version store type; ignored otherwise.
jdbc:
  # -- The JDBC connection string. If you are using Nessie OSS images, then only
  # PostgreSQL, MariaDB and MySQL URLs are supported. Check your JDBC driver documentation
  # for the correct URL format.
  jdbcUrl: jdbc:postgresql://localhost:5432/my_database?currentSchema=nessie
  secret:
    # -- The secret name to pull datasource credentials from.
    name: datasource-creds
    # -- The secret key storing the datasource username.
    username: username
    # -- The secret key storing the datasource password.
    password: password

# BigTable settings. Only required when using BIGTABLE version store type; ignored otherwise.
bigtable:
  # -- The Google Cloud project ID.
  projectId: my-gcp-project
  # -- The Google Cloud Bigtable instance ID.
  instanceId: nessie-bigtable
  # -- The Google Cloud Bigtable app profile ID.
  appProfileId: default
  # -- The secret to use to authenticate against BigTable.
  # When provided, it is assumed that authentication will use a service account JSON key.
  # See https://cloud.google.com/iam/docs/keys-create-delete for details on how to create a
  # service account key.
  # If left empty, then Workload Identity usage is assumed instead; in this case, make sure that
  # the pod's service account has been granted access to BigTable.
  # See https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity#authenticating_to
  # for details on how to create a suitable service account.
  # Important: when using Workload Identity, unless the cluster is in Autopilot mode, it is also
  # required to add the following nodeSelector label:
  # iam.gke.io/gke-metadata-server-enabled: "true"
  # This is not done automatically by this chart because this selector would be invalid for
  # Autopilot clusters.
  secret: {}
    # # -- The secret name to pull a valid Google Cloud service account key from.
    # name: bigtable-creds
    # # -- The secret key storing the Google Cloud service account JSON key.
    # key: sa_json

# -- The Nessie catalog server configuration.
catalog:

  # -- Whether to enable the REST catalog service.
  enabled: false

  # -- Iceberg catalog settings.
  iceberg:

    # -- The default warehouse name. Required. This is just a symbolic name; it must refer to a
    # declared warehouse below.
    defaultWarehouse: ~  # warehouse1

    # -- Iceberg config defaults applicable to all clients and warehouses. Any properties that are
    # common to all iceberg clients should be included here. They will be passed to all clients on
    # all warehouses as config defaults. These defaults can be overridden on a per-warehouse basis,
    # see below.
    configDefaults: {}
    # io-impl: org.apache.iceberg.hadoop.HadoopFileIO

    # -- Iceberg config overrides applicable to all clients and warehouses. Any properties that are
    # common to all iceberg clients should be included here. They will be passed to all clients on
    # all warehouses as config overrides. These overrides can be overridden on a per-warehouse
    # basis, see below.
    configOverrides: {}
    # s3.acl: public-read-write

    # -- Iceberg warehouses. Each warehouse is a location where Iceberg tables are stored. Each
    # warehouse has a name, a location, and optional config defaults and overrides. At least one
    # warehouse must be defined.
    warehouses:
      # -- Symbolic name of the warehouse. Required.
    - name: ~  # warehouse1
      # -- Location of the warehouse. Required. Used to determine the base location of a table.
      # Scheme must be either s3 (Amazon S3), gs (Google GCS) or abfs / abfss (Azure ADLS). Storage
      # properties for each location can be defined below.
      location: ~  # s3://bucket1/
      # -- Iceberg config defaults specific to this warehouse. They override any defaults specified
      # above in catalog.iceberg.configDefaults.
      configDefaults: {}
      # -- Iceberg config overrides specific to this warehouse. They override any defaults specified
      # above in catalog.iceberg.configOverrides.
      configOverrides: {}
    # In rare cases it might be legit to turn off the object-stores readiness check.
    objectStoresHealthCheckEnabled: true

  # -- Catalog storage settings.
  storage:
    # -- Interval after which a request is retried when Storage responds with some "retry later"
    # error. Must be a valid ISO duration.
    retryAfter: ~

    s3:

      # Global S3 settings. Can be overridden on a per-bucket basis below.
      defaultOptions:
        # -- DNS name of the region, required for AWS.
        region: ~  # us-west-2
        # -- Endpoint URI, required for private clouds. Optional; if not provided, the default is
        # used.
        endpoint: ~  # "https://bucket1.s3.amazonaws.com"
        # -- Endpoint URI, required for private clouds. Optional; if not provided, the default is
        # used. If the endpoint URIs for the Nessie server and clients differ, this one defines the
        # endpoint used for the Nessie server.
        externalEndpoint: ~
        # -- Whether to use path-style access. Optional; if not provided, the default is used. If
        # true, path-style access will be used, as in: https://<domain>/<bucket>. If false, a
        # virtual-hosted style will be used instead, as in: https://<bucket>.<domain>.
        pathStyleAccess: ~
        # -- AWS Access point for this bucket. Access points can be used to perform S3 operations by
        # specifying a mapping of bucket to access points. This is useful for multi-region access,
        # cross-region access, disaster recovery, etc. See
        # https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html.
        accessPoint: ~
        # -- Authorize cross-region calls when contacting an access point. The default is false.
        allowCrossRegionAccessPoint: ~
        # -- Controls the authentication mode for the Catalog server. Valid values are:
        # - APPLICATION_GLOBAL: Use the default AWS credentials provider chain.
        # - STATIC: Static credentials provided through the accessKeySecret option.
        # The default is STATIC.
        authType: ~  # STATIC
        # --Optional parameter to disable S3 request signing. Default is to enable S3 request signing.
        requestSigningEnabled: ~  # true
        # -- The STS endpoint. Optional; if not provided, the default is used. This parameter must
        # be set if the cloud provider is not AMAZON and the catalog is configured to use S3
        # sessions (e.g. to use the "assume role" functionality).
        stsEndpoint: ~  # "https://sts.amazonaws.com"

        clientIam:
          # -- Whether to enable vended credentials functionality. If this option is enabled, the
          # server will temporarily assume the configured role, then pass the returned session
          # credentials down to the client, for each table that is created, updated or registered.
          # Vended credentials are not cached server-side.
          enabled: ~  # false
          # -- The ARN of the role to assume for accessing S3 data. This parameter is required for
          # Amazon S3, but may not be required for other storage providers (e.g. Minio does not use it
          # at all).
          roleArn: ~  # "arn:aws:iam::123456789012:role/role-name"
          # -- The IAM policy in JSON format to be used as an inline session policy when calling the
          # assume-role endpoint. Optional.
          policy: ~  # "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"*\" } ] }"
          # -- An identifier for the assumed role session. This parameter is most important in cases
          # when the same role is assumed by different principals in different use cases.
          roleSessionName: ~  # nessie
          # -- An identifier for the party assuming the role. This parameter must match the external
          # ID configured in IAM rules that govern the assume role process for the specified roleArn.
          externalId: ~
          # -- A higher bound estimate of the expected duration of client "sessions" working with data in
          # this bucket. A session, for example, is the lifetime of an Iceberg REST catalog object on
          # the client side. This value is used for validating expiration times of credentials
          # associated with the warehouse. If unset, a default of one hour is assumed.
          sessionDuration: ~
          # -- Additional IAM policy statements in JSON format to add to generated per-table IAM policies.
          statements: ~
          # - >-
          #   {
          #   "Effect": "Allow",
          #   "Action": "s3:GetObject",
          #   "Resource": "arn:aws:s3:::bucket1.{{ .Release.Namespace }}/*"
          #   }
          # - >-
          #   {
          #     "Effect": "Allow",
          #     "Action": "s3:PutObject",
          #     "Resource": "arn:aws:s3:::bucket1.{{ .Release.Namespace }}/*"
          #   }

        # -- Settings only relevant when clientAuthenticationMode is ASSUME_ROLE.
        serverIam:
          # -- Whether to enable server assume-role functionality. If this option is enabled, the
          # server will attempt to assume the configured role at startup and cache the returned
          # session credentials.
          enabled: ~  # false
          # -- The ARN of the role to assume for accessing S3 data. This parameter is required for
          # Amazon S3, but may not be required for other storage providers (e.g. Minio does not use it
          # at all).
          roleArn: ~  # "arn:aws:iam::123456789012:role/role-name"
          # -- The IAM policy in JSON format to be used as an inline session policy when calling the
          # assume-role endpoint. Optional.
          policy: ~  # "{ \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:*\", \"Resource\": \"*\" } ] }"
          # -- An identifier for the assumed role session. This parameter is most important in cases
          # when the same role is assumed by different principals in different use cases.
          roleSessionName: ~  # nessie
          # -- An identifier for the party assuming the role. This parameter must match the external
          # ID configured in IAM rules that govern the assume role process for the specified roleArn.
          externalId: ~
          # -- A higher bound estimate of the expected duration of client "sessions" working with data in
          # this bucket. A session, for example, is the lifetime of an Iceberg REST catalog object on
          # the client side. This value is used for validating expiration times of credentials
          # associated with the warehouse. If unset, a default of one hour is assumed.
          sessionDuration: ~

        # -- AWS credentials. Required when serverAuthenticationMode is STATIC.
        accessKeySecret:
          # -- The secret name to pull AWS credentials from.
          name: ~
          # -- The secret key storing the AWS secret key id.
          awsAccessKeyId: ~
          # -- The secret key storing the AWS secret access key.
          awsSecretAccessKey: ~

      # -- Per-bucket S3 settings. Override the general settings above.
      buckets: []
      # - name: bucket1
      #   authority: bucket1
      #   pathPrefix: path/in/the/bucket
      #   endpoint: "https://bucket1.s3.amazonaws.com"
      #   accessKeySecret:
      #     name: awscreds
      #     awsAccessKeyId: aws_access_key_id
      #     awsSecretAccessKey: aws_secret_access_key

      # -- S3 transport settings. Not overridable on a per-bucket basis.
      transport:
        # -- Override the default maximum number of pooled connections.
        maxHttpConnections: ~
        # -- Override the default connection read timeout. Must be a valid ISO duration.
        readTimeout: ~
        # -- Override the default TCP connect timeout. Must be a valid ISO duration.
        connectTimeout: ~
        # -- Override default connection acquisition timeout. This is the time a request will wait
        # for a connection from the pool. Must be a valid ISO duration.
        connectionAcquisitionTimeout: ~
        # -- Override default max idle time of a pooled connection. Must be a valid ISO duration.
        connectionMaxIdleTime: ~
        # -- Override default time-time of a pooled connection. Must be a valid ISO duration.
        connectionTimeToLive: ~
        # -- Override default behavior whether to expect an HTTP/100-Continue. Must be a valid ISO
        # duration.
        expectContinueEnabled: ~

      sessionCredentials:
        # -- The time period to subtract from the S3 session credentials (assumed role credentials)
        # expiry time to define the time when those credentials become eligible for refreshing.
        # Not overridable on a per-bucket basis. The default is PT5M (5 minutes).
        sessionCredentialRefreshGracePeriod: ~  # PT5M
        # -- Maximum number of entries to keep in the session credentials cache (assumed role
        # credentials). Not overridable on a per-bucket basis. The default is 1000.
        sessionCredentialCacheMaxEntries: ~  # 1000
        # -- Maximum number of entries to keep in the STS clients cache. Not overridable on a
        # per-bucket basis. The default is 50.
        stsClientsCacheMaxEntries: ~  # 50

    gcs:

      # Global GCS settings. Can be overridden on a per-bucket basis below.
      defaultOptions:
        # -- The default endpoint override to use. The endpoint is almost always used for testing
        # purposes. If the endpoint URIs for the Nessie server and clients differ, this one defines
        # the endpoint used for the Nessie server.
        host: ~
        # -- When using a specific endpoint, see host, and the endpoint URIs for the Nessie server
        # differ, you can specify the URI passed down to clients using this setting. Otherwise,
        # clients will receive the value from the host setting.
        externalHost: ~
        # -- Optionally specify the user project (Google term).
        userProject: ~
        # --  The Google project ID.
        projectId: ~
        # -- The Google quota project ID.
        quotaProjectId: ~
        # -- The Google client lib token.
        clientLibToken: ~
        # -- The authentication type to use. Valid values are: NONE, USER, SERVICE_ACCOUNT,
        # ACCESS_TOKEN, APPLICATION_DEFAULT. The default is NONE.
        authType: ~

        # -- The Google Cloud service account key secret. This is required when authType is USER or
        # SERVICE_ACCOUNT.
        authCredentialsJsonSecret:
          # -- The secret name to pull a valid Google Cloud service account key from.
          name: ~
          # -- The secret key storing the Google Cloud service account JSON key.
          key: ~

        # -- The oauth2 token secret. This is required when authType is ACCESS_TOKEN.
        oauth2TokenSecret:
          # # -- The secret name to pull a valid Google Cloud service account key from.
          name: ~
          # # -- The secret key storing the token.
          token: ~
          # # -- The secret key storing the token's expiresAt value (optional).
          expiresAt: ~

        # -- Customer-supplied AES256 key for blob encryption when writing. Currently unsupported.
        encryptionKey: ~
        # -- Customer-supplied AES256 key for blob decryption when reading. Currently unsupported.
        decryptionKey: ~

        # -- The read chunk size in bytes. Must be a valid ISO duration.
        readChunkSize: ~
        # -- The write chunk size in bytes. Must be a valid ISO duration.
        writeChunkSize: ~
        # -- The delete batch size.
        deleteBatchSize: ~

      # -- Per-bucket GCS settings. Override the general settings above.
      buckets: []
      # - name: bucket1
      #   authority: bucket1
      #   pathPrefix: path/in/the/bucket
      #   authType: ACCESS_TOKEN
      #   oauth2TokenSecret:
      #     name: gcs-creds
      #     key: token
      #     expiresAt: expiresAt

      # -- GCS transport settings. Not overridable on a per-bucket basis.
      transport:
        # -- Override the default maximum number of attempts.
        maxAttempts: ~
        # -- Override the default connection timeout. Must be a valid ISO duration.
        connectTimeout: ~
        # -- Override the default read timeout. Must be a valid ISO duration.
        readTimeout: ~
        # -- Override the default initial retry delay. Must be a valid ISO duration.
        initialRetryDelay: ~
        # -- Override the default maximum retry delay. Must be a valid ISO duration.
        maxRetryDelay: ~
        # -- Override the default retry delay multiplier. Must be a valid ISO duration.
        retryDelayMultiplier: ~
        # -- Override the default initial RPC timeout. Must be a valid ISO duration.
        initialRpcTimeout: ~
        # -- Override the default maximum RPC timeout. Must be a valid ISO duration.
        maxRpcTimeout: ~
        # -- Override the default RPC timeout multiplier. Must be a valid ISO duration.
        rpcTimeoutMultiplier: ~
        # -- Override the default logical request timeout. Must be a valid ISO duration.
        logicalTimeout: ~
        # -- Override the default total timeout. Must be a valid ISO duration.
        totalTimeout: ~

    adls:

      # Global ADLS settings. Can be overridden on a per-filesystem basis below.
      defaultOptions:
        # -- Custom HTTP endpoint. In case clients need to use a different URI, use externalEndpoint.
        endpoint: ~
        # -- Custom HTTP endpoint to be used by clients. If not set, the endpoint value is used.
        externalEndpoint: ~
        # -- The retry strategy to use. Valid values are: NONE, EXPONENTIAL_BACKOFF, FIXED_DELAY.
        # The default is EXPONENTIAL_BACKOFF.
        retryPolicy: ~
        # -- The maximum number of retries. Must be a positive integer. Default is 4. Optional.
        # Valid if retryPolicy is EXPONENTIAL_BACKOFF or FIXED_DELAY.
        maxRetries: ~
        # -- The maximum time allowed before a request is cancelled and assumed failed, default is
        # Integer.MAX_VALUE. Optional. Must be a valid ISO duration. Valid if retryPolicy is
        # EXPONENTIAL_BACKOFF or FIXED_DELAY.
        tryTimeout: ~
        # -- Specifies the amount of delay to use before retrying an operation, default value is
        # PT4S (4 seconds) when retryPolicy is EXPONENTIAL_BACKOFF and PT30S (30 seconds) when
        # retryPolicy is FIXED_DELAY. Must be a valid ISO duration.
        retryDelay: ~
        # --  Specifies the maximum delay allowed before retrying an operation, default value is
        # PT120s (120 seconds). Must be a valid ISO duration. Valid if retryPolicy is
        # EXPONENTIAL_BACKOFF.
        maxRetryDelay: ~
        # -- The authentication type to use. Valid values are:  NONE, STORAGE_SHARED_KEY, SAS_TOKEN,
        # APPLICATION_DEFAULT. The default is NONE.
        authType: ~
        # -- A secret containing the account name and key to use. Required when authType is
        # STORAGE_SHARED_KEY.
        accountSecret:
          # -- Name of the secret containing the account name and key.
          name: ~
          # -- Secret key containing the fully-qualified account name, e.g. "myaccount.dfs.core.windows.net".
          accountName: ~
          # -- Secret key containing the account key.
          accountKey: ~
        # -- A secret containing the SAS token to use. Required when authType is SAS_TOKEN.
        sasTokenSecret:
          # -- Name of the secret containing the SAS token.
          name: ~
          # -- Secret key containing the SAS token.
          sasToken: ~

      # -- Per-filesystem ADLS settings. Override the general settings above.
      filesystems: []
      # - name: filesystem1
      #   authority: bucket1
      #   pathPrefix: path/in/the/bucket
      #   endpoint: http://localhost/adlsgen2/bucket
      #   accountSecret:
      #     name: adls-account-secret
      #     accountName: accountName
      #     accountKey: accountKeyRef

      # -- ADLS transport settings. Not overridable on a per-bucket basis.
      transport:
        # -- The default maximum connection pool size is determined by the underlying HTTP client.
        # Not overridable on a per-filesystem basis.
        maxHttpConnections: ~
        # -- Sets the connection timeout for a request to be sent. The default is PT10S (10
        # seconds). Must be a valid ISO duration. Not overridable on a per-filesystem basis.
        connectTimeout: ~
        # -- Sets the read timeout duration used when reading the server response. The default is
        # PT60S (60 seconds). Must be a valid ISO duration. Not overridable on a per-filesystem
        # basis.
        readTimeout: ~
        # -- Sets the write timeout duration used when writing the request to the server. The
        # default is PT60S (60 seconds). Must be a valid ISO duration. Not overridable on a
        # per-filesystem basis.
        writeTimeout: ~
        # -- Sets the maximum idle time for a connection to be kept alive. The default is PT60S (60
        # seconds). Must be a valid ISO duration. Not overridable on a per-filesystem basis.
        connectionIdleTimeout: ~
        # -- The size of each data chunk returned from the service in bytes. The default value is 4
        # MB. Not overridable on a per-filesystem basis.
        readBlockSize: ~
        # -- Sets the block size in bytes to transfer at a time. Not overridable on a per-filesystem
        # basis.
        writeBlockSize: ~

      # -- Custom ADLS configuration options, see javadocs of com.azure.core.util.Configuration.
      # Not overridable on a per-filesystem basis.
      advancedConfig: {}


# -- Advanced configuration.
# You can pass here any valid Nessie or Quarkus configuration property.
# Any property that is defined here takes precedence over all the other configuration values generated by this chart.
# Properties can be passed "flattened" or as nested YAML objects (see examples below).
advancedConfig:
  {}
# Nessie version store settings
# -----------------------------
#
#  See description of the various cache size parameters and their defaults.
#
#  nessie.version.store.persist.cache-capacity-mb: (defaults to fractional size, based on max-heap size)
#  nessie.version.store.persist.cache-capacity-fraction-of-heap: 0.7
#  nessie.version.store.persist.cache-capacity-fraction-adjust-mb: 256
#  nessie.version.store.persist.cache-capacity-fraction-min-size-mb: 64
#
#  nessie.server.default-branch: my-branch
#
#  nessie.version.store.persist.repository-id: my-repository
#
# Reverse Proxy Settings
# ----------------------
#
# These config options are mentioned only for documentation purposes. Consult the
# Quarkus documentation for "Running behind a reverse proxy" and configure those
# depending on your actual needs.
#
# See https://quarkus.io/guides/http-reference#reverse-proxy
#
# Do NOT enable these option unless your reverse proxy (for example istio or nginx)
# is properly setup to set these headers but also filter those from incoming requests.
#
#  quarkus:
#   http:
#     proxy:
#       proxy-address-forwarding: "true"
#       allow-x-forwarded: "true"
#       enable-forwarded-host: "true"
#       enable-forwarded-prefix: "true"
#       trusted-proxies: "127.0.0.1"

# -- Advanced configuration via Environment Variables.
# Extra environment variables to add to the Nessie server container.
# You can pass here any valid EnvVar object:
# https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#envvar-v1-core
# This can be useful to get configuration values from Kubernetes secrets or config maps.
extraEnv:
  []
#  - name: QUARKUS_MONGODB_APPLICATION_NAME
#    value: my-app
#  - name: QUARKUS_MONGODB_TLS
#    valueFrom:
#      configMapKeyRef:
#        name: mongodb-config
#        key: tls

authentication:
  # -- Specifies whether authentication for the nessie server should be enabled.
  enabled: false
  # -- Sets the base URL of the OpenID Connect (OIDC) server. Required if authentication is enabled (unless local token introspection is enforced through advanced configuration).
  oidcAuthServerUrl: ~  # http://example.com:8080/auth/realms/master
  # -- Set the OIDC client ID. If Nessie must contact the OIDC server, this is the client ID that will be used to identify the application.
  oidcClientId: nessie
  # -- Set the OIDC client secret. Whether the client secret is required depends on the OIDC server configuration.
  # For Keycloak, the client secret is generally not required as the returned tokens can be introspected locally by Nessie.
  # If token introspection requires a round-trip to the OIDC server, the client secret is required.
  oidcClientSecret: {}
#    name: nessie-oidc-creds
#    key: client-secret

authorization:
  # -- Specifies whether authorization for the nessie server should be enabled.
  enabled: false
  # -- The authorization rules when authorization.enabled=true. Example rules can be found at https://projectnessie.org/features/metadata_authorization/#authorization-rules
  rules:
    {}
    # allowViewingBranch: op=='VIEW_REFERENCE' && role.startsWith('test_user') && ref.startsWith('allowedBranch')
    # allowCommits: op=='COMMIT_CHANGE_AGAINST_REFERENCE' && role.startsWith('test_user') && ref.startsWith('allowedBranch')

tracing:
  # -- Specifies whether tracing for the nessie server should be enabled.
  enabled: false
  # -- The collector endpoint URL to connect to (required).
  # The endpoint URL must have either the http:// or the https:// scheme.
  # The collector must talk the OpenTelemetry protocol (OTLP) and the port must be its gRPC port (by default 4317).
  # See https://quarkus.io/guides/opentelemetry for more information.
  endpoint: "http://otlp-collector:4317"
  # -- Which requests should be sampled. Valid values are: "all", "none", or a ratio between 0.0 and
  # "1.0d" (inclusive). E.g. "0.5d" means that 50% of the requests will be sampled.
  sample: "1.0d"
  # -- Resource attributes to identify the nessie service among other tracing sources.
  # See https://opentelemetry.io/docs/reference/specification/resource/semantic_conventions/#service.
  # If left empty, traces will be attached to a service named "Nessie"; to change this, provide a service.name attribute here.
  attributes:
    {}
    # service.name: my-nessie

metrics:
  # -- Specifies whether metrics for the nessie server should be enabled.
  enabled: true
  # -- Additional tags (dimensional labels) to add to the metrics.
  tags:
    {}
    # service: nessie
    # environment: production

serviceMonitor:
  # -- Specifies whether a ServiceMonitor for Prometheus operator should be created.
  enabled: true
  # -- The scrape interval; leave empty to let Prometheus decide. Must be a valid duration, e.g. 1d, 1h30m, 5m, 10s.
  interval: ""
  # -- Labels for the created ServiceMonitor so that Prometheus operator can properly pick it up.
  labels:
    {}
    # release: prometheus
  # -- Relabeling rules to apply to metrics. Ref https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config.
  metricRelabelings: []
    # - source_labels: [ __meta_kubernetes_namespace ]
    #   separator: ;
    #   regex: (.*)
    #   target_label: namespace
    #   replacement: $1
    #   action: replace

serviceAccount:
  # -- Specifies whether a service account should be created.
  create: true
  # -- Annotations to add to the service account.
  annotations: {}
  # -- The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template.
  name: ""

# -- Annotations to apply to nessie pods.
podAnnotations: {}

# -- Additional Labels to apply to nessie pods.
podLabels: {}

# -- Additional Labels to apply to nessie configmap.
configMapLabels: {}

# -- Additional Annotations to apply to nessie configmap.
configMapAnnotations: {}

# -- Security context for the nessie pod. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/.
podSecurityContext:
  # GID 10001 is compatible with Nessie OSS default images starting with 0.95.1; change this if you
  # are using a different image.
  fsGroup: 10001
  seccompProfile:
    type: RuntimeDefault

# -- Security context for the nessie container. See https://kubernetes.io/docs/tasks/configure-pod-container/security-context/.
securityContext:
  # UID 10000 and GID 10001 are compatible with Nessie OSS default images starting with 0.95.1;
  # change this if you are using a different image.
  runAsUser: 10000
  runAsGroup: 10001
  runAsNonRoot: true
  privileged: false
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
      - ALL

# -- Nessie main service settings.
service:
  # -- The type of service to create.
  type: ClusterIP
  # -- The ports the service will listen on.
  # At least one port is required; the first port implicitly becomes the HTTP port that the
  # application will use for serving API requests. By default, it's 19120.
  # Note: port names must be unique and no more than 15 characters long.
  ports:
    - name: nessie-http
      number: 19120
    # - name: nessie-https
    #  number: 19121
  # -- The session affinity for the service. Valid values are: None, ClientIP.
  # ClientIP enables sticky sessions based on the client's IP address.
  # This is generally beneficial to Nessie deployments, but some testing may be
  # required in order to make sure that the load is distributed evenly among the pods.
  # Also, this setting affects only internal clients, not external ones.
  # If Ingress is enabled, it is recommended to set sessionAffinity to None.
  sessionAffinity: None
  # -- You can specify your own cluster IP address
  # If you define a Service that has the .spec.clusterIP set to "None" then Kubernetes does not assign an IP address.
  # Instead, DNS records for the service will return the IP addresses of each pod targeted by the server. This is
  # called a headless service.
  # See https://kubernetes.io/docs/concepts/services-networking/service/#headless-services
  clusterIP: ""
  # -- The traffic policy fields control how traffic from internal and external sources are routed respectively.
  # Valid values are Cluster and Local.
  # Set the field to Cluster to route traffic to all ready endpoints.
  # Set the field to Local to only route to ready node-local endpoints.
  # If the traffic policy is Local and there are no node-local endpoints, traffic is dropped by kube-proxy
  internalTrafficPolicy: Cluster
  externalTrafficPolicy: Cluster
  # -- The traffic distribution field provides another way to influence traffic routing within a Kubernetes Service.
  # While traffic policies focus on strict semantic guarantees, traffic distribution allows you to express preferences
  # such as routing to topologically closer endpoints.
  # Valid values are: PreferClose
  trafficDistribution: PreferClose
  # -- Annotations to add to the service.
  annotations: {}

# -- Management service settings. These settings are used to configure liveness and readiness probes,
# and to configure the dedicated headless service that will expose health checks and metrics, e.g.
# for metrics scraping and service monitoring.
managementService:
  # -- The name of the management port. Required.
  portName: nessie-mgmt
  # -- The port the management service listens on. By default, the management interface is exposed
  # on HTTP port 9000.
  portNumber: 9000
  # -- Annotations to add to the service.
  annotations: {}

# -- Additional service definitions. All service definitions always select all Nessie pods. Use
# this if you need to expose specific ports with different configurations.
extraServices: []
  #  - # -- The suffix to append to the service name. Required.
  #    nameSuffix: "-ext"
  #    # -- The type of service to create.
  #    type: LoadBalancer
  #    # -- The ports the service will listen on.
  #    ports:
  #    - name: nessie-http
  #      number: 19120
  #    - name: nessie-https
  #      number: 19121
  #    sessionAffinity: None
  #    clusterIP: ""
  #    internalTrafficPolicy: Cluster
  #    externalTrafficPolicy: Cluster
  #    trafficDistribution: PreferClose
  #    annotations: {}

# -- Nessie Ingress settings.
# These settings generate an Ingress resource that routes external traffic to the Nessie service.
# Consider enabling sticky sessions based on the remote client's IP address;
# this is generally beneficial to Nessie deployments, but some testing may be
# required in order to make sure that the load is distributed evenly among the pods.
# Check your ingress controller's documentation.
ingress:
  # -- Specifies whether an ingress should be created.
  enabled: false
  # -- Specifies the ingressClassName; leave empty if you don't want to customize it.
  className: ""
  # -- Annotations to add to the ingress.
  annotations: {
    # nginx.ingress.kubernetes.io/upstream-hash-by: "$binary_remote_addr"
  }
  # -- Specifies the path type of host paths. Valid values are: "Prefix", "Exact" or "ImplementationSpecific".
  pathType: ImplementationSpecific
  # -- A list of host paths used to configure the ingress.
  hosts:
    - host: chart-example.local
      paths: []
      # -- The service target for the ingress.
      service:
        # -- The port name to route traffic to. Must match one of the ports in service.ports or in
        # extraServices.ports. Optional; if not provided, the first port in service.ports will be used.
        portName: nessie-http
        # -- The target service name suffix. Optional; if not provided, the main service will be
        # targeted. Change this only if you are targeting a service defined in extraServices.
        nameSuffix: ""
  # -- A list of TLS certificates; each entry has a list of hosts in the certificate,
  # along with the secret name used to terminate TLS traffic on port 443.
  tls: []
#    - hosts:
#        - chart-example1.local
#        - chart-example2.local
#      secretName: secret1

# -- Override the strategy for nessie deployment.
# Valid values for type are: RollingUpdate and Recreate.
# If you are using the ROCKSDB version store type then you should use Recreate.
# Max Surge will allow new pods to be created before old ones are culled. Do not enable this when using ROCKSDB
# version store type.
# Max Unavailable will allow old pods to be culled before replacements are created
# See: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
deploymentStrategy:
  {}
  # type: RollingUpdate
  # rollingUpdate:
  #   maxUnavailable: 0
  #   maxSurge: 1

# -- Configures the resources requests and limits for nessie pods.
# We usually recommend not to specify default resources and to leave this as a conscious
# choice for the user. This also increases chances charts run on environments with little
# resources, such as Minikube. If you do want to specify resources, uncomment the following
# lines, adjust them as necessary, and remove the curly braces after 'resources:'.
resources:
  {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

autoscaling:
  # -- Specifies whether automatic horizontal scaling should be enabled.
  # Do not enable this when using ROCKSDB version store type.
  enabled: false
  # -- The minimum number of replicas to maintain.
  minReplicas: 1
  # -- The maximum number of replicas to maintain.
  maxReplicas: 3
  # -- Optional; set to zero or empty to disable.
  targetCPUUtilizationPercentage: 80
  # -- Optional; set to zero or empty to disable.
  targetMemoryUtilizationPercentage:

# -- Node labels which must match for the nessie pod to be scheduled on that node. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector.
nodeSelector:
  {}
  # kubernetes.io/os: linux

# -- A list of tolerations to apply to nessie pods. See https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/.
tolerations: []
#  - key: "node-role.kubernetes.io/control-plane"
#    operator: "Exists"
#    effect: "NoSchedule"

# -- Affinity and anti-affinity for nessie pods. See https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity.
affinity: {}
#  podAffinity:
#    preferredDuringSchedulingIgnoredDuringExecution:
#      - weight: 100
#        podAffinityTerm:
#          topologyKey: kubernetes.io/hostname
#          labelSelector:
#            matchExpressions:
#              - key: app.kubernetes.io/name
#                operator: In
#                values:
#                  - nessie

# -- Configures the liveness probe for nessie pods.
livenessProbe:
  # -- Number of seconds after the container has started before liveness probes are initiated. Minimum value is 0.
  initialDelaySeconds: 5
  # -- How often (in seconds) to perform the probe. Minimum value is 1.
  periodSeconds: 10
  # -- Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1.
  successThreshold: 1
  # -- Minimum consecutive failures for the probe to be considered failed after having succeeded. Minimum value is 1.
  failureThreshold: 3
  # -- Number of seconds after which the probe times out. Minimum value is 1.
  timeoutSeconds: 10
  # -- Optional duration in seconds the pod needs to terminate gracefully upon probe failure. Minimum value is 1.
  terminationGracePeriodSeconds: 30

# -- Configures the readiness probe for nessie pods.
readinessProbe:
  # -- Number of seconds after the container has started before readiness probes are initiated. Minimum value is 0.
  initialDelaySeconds: 5
  # -- How often (in seconds) to perform the probe. Minimum value is 1.
  periodSeconds: 10
  # -- Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1.
  successThreshold: 1
  # -- Minimum consecutive failures for the probe to be considered failed after having succeeded. Minimum value is 1.
  failureThreshold: 3
  # -- Number of seconds after which the probe times out. Minimum value is 1.
  timeoutSeconds: 10

# -- Extra volumes to add to the nessie pod. See https://kubernetes.io/docs/concepts/storage/volumes/.
extraVolumes: []
    # - name: extra-volume
    #   emptyDir: {}

# -- Extra volume mounts to add to the nessie container. See https://kubernetes.io/docs/concepts/storage/volumes/.
extraVolumeMounts: []
    # - name: extra-volume
    #   mountPath: /usr/share/extra-volume

# -- Add additional init containers to the nessie pod(s) See https://kubernetes.io/docs/concepts/workloads/pods/init-containers/.
extraInitContainers: []
    # - name: your-image-name
    #   image: your-image
    #   imagePullPolicy: Always
    #   command: ['sh', '-c', 'echo "hello world"']
