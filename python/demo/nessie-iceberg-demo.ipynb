{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessie Demo\n",
    "===========\n",
    "This demo showcases how to use Nessie python API along with Spark3 from Iceberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Pyspark + Nessie environment\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars\", \"../../clients/iceberg/spark3/target/nessie-iceberg-spark3-0.2.0-SNAPSHOT.jar\") \\\n",
    "                    .config(\"spark.sql.execution.pyarrow.enabled\", \"true\") \\\n",
    "                    .config(\"spark.hadoop.fs.defaultFS\", 'file://' + os.getcwd() + '/spark_warehouse') \\\n",
    "                    .config(\"spark.hadoop.nessie.url\", \"http://localhost:19120/api/v1\") \\\n",
    "                    .config(\"spark.hadoop.nessie.ref\", \"main\") \\\n",
    "                    .config(\"spark.sql.catalog.nessie\", \"com.dremio.nessie.iceberg.spark.NessieIcebergSparkCatalog\") \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "jvm = sc._gateway.jvm\n",
    "\n",
    "java_import(jvm, \"com.dremio.nessie.iceberg.NessieCatalog\")\n",
    "java_import(jvm, \"org.apache.iceberg.catalog.TableIdentifier\")\n",
    "java_import(jvm, \"org.apache.iceberg.Schema\")\n",
    "java_import(jvm, \"org.apache.iceberg.types.Types\")\n",
    "java_import(jvm, \"org.apache.iceberg.PartitionSpec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up nessie branches\n",
    "----------------------------\n",
    "\n",
    "- Branch `main` already exists\n",
    "- Create branch `dev`\n",
    "- List all branches (pipe JSON result into jq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tables under dev branch\n",
    "-------------------------------------\n",
    "\n",
    "Creating two tables under the `dev` branch:\n",
    "- region\n",
    "- nation\n",
    "\n",
    "It is not yet possible to create table using pyspark and iceberg, so Java code is used instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"nessie.ref\", \"dev\")\n",
    "catalog = jvm.NessieCatalog(sc._jsc.hadoopConfiguration())\n",
    "\n",
    "# Creating region table\n",
    "region_name = jvm.TableIdentifier.parse(\"testing.region\")\n",
    "region_schema = jvm.Schema([\n",
    "    jvm.Types.NestedField.optional(1, \"R_REGIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(2, \"R_NAME\", jvm.Types.StringType.get()),\n",
    "    jvm.Types.NestedField.optional(3, \"R_COMMENT\", jvm.Types.StringType.get()),\n",
    "])\n",
    "region_spec = jvm.PartitionSpec.unpartitioned()\n",
    "\n",
    "region_table = catalog.createTable(region_name, region_schema, region_spec)\n",
    "region_df = spark.read.load(\"data/region.parquet\")\n",
    "region_df.write.option('hadoop.nessie.ref', 'dev').format(\"iceberg\").mode(\"overwrite\").save(\"testing.region\")\n",
    "\n",
    "# Creating nation table\n",
    "nation_name = jvm.TableIdentifier.parse(\"testing.nation\")\n",
    "nation_schema = jvm.Schema([\n",
    "    jvm.Types.NestedField.optional(1, \"N_NATIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(2, \"N_NAME\", jvm.Types.StringType.get()),\n",
    "    jvm.Types.NestedField.optional(3, \"N_REGIONKEY\", jvm.Types.LongType.get()),\n",
    "    jvm.Types.NestedField.optional(4, \"N_COMMENT\", jvm.Types.StringType.get()),\n",
    "])\n",
    "nation_spec = jvm.PartitionSpec.builderFor(nation_schema).truncate(\"N_NAME\", 2).build()\n",
    "nation_table = catalog.createTable(nation_name, nation_schema, nation_spec)\n",
    "\n",
    "nation_df = spark.read.load(\"data/nation.parquet\")\n",
    "nation_df.write.option('hadoop.nessie.ref', 'dev').format(\"iceberg\").mode(\"overwrite\").save(\"testing.nation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check generated tables\n",
    "----------------------------\n",
    "   \n",
    "Check tables generated under the dev branch (and that the main branch does not have any tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list --ref dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `dev` and `main` branches point to different commits now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev promotion\n",
    "-------------\n",
    "\n",
    "Promote dev branch promotion to main.\n",
    "\n",
    "* main now has the same tables as dev\n",
    "* main and dev point to the same commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch main dev --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie --verbose branch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `etl` branch\n",
    "----------------------\n",
    "\n",
    "- Create a branch `etl` out of `main`\n",
    "- add data to nation\n",
    "- alter the schema of region\n",
    "- create table city\n",
    "- query the tables in `etl`\n",
    "- query the tables in `main`\n",
    "- promote `etl` branch to `main`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch etl main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nation = Row(\"N_NATIONKEY\", \"N_NAME\", \"N_REGIONKEY\", \"N_COMMENT\")\n",
    "new_nations = spark.createDataFrame([\n",
    "    Nation(25, \"SYLDAVIA\", 3, \"King Ottokar's Sceptre\"),\n",
    "    Nation(26, \"SAN THEODOROS\", 1, \"The Picaros\")])\n",
    "new_nations.write.option('hadoop.nessie.ref', 'etl').format(\"iceberg\").mode(\"append\").save(\"testing.nation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'etl')\n",
    "\n",
    "etl_catalog = jvm.NessieCatalog(hadoop_conf)\n",
    "etl_catalog.loadTable(region_name).updateSchema().addColumn('R_ABBREV', jvm.Types.StringType.get()).commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating city table\n",
    "sc.getConf().set(\"spark.hadoop.nessie.ref\", \"etl\")\n",
    "spark.sql(\"CREATE TABLE nessie.testing.city (C_CITYKEY BIGINT, C_NAME STRING, N_NATIONKEY BIGINT, C_COMMNT STRING) USING iceberg PARTITIONED BY (N_NATIONKEY)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynessie import init\n",
    "nessie = init()\n",
    "nessie.list_keys('main').entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i.name for i in nessie.list_keys('etl').entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nessie.assign_branch('main','etl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{i.name:i.hash_ for i in nessie.list_references()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `experiment` branch\n",
    "--------------------------------\n",
    "\n",
    "- create `experiment` branch from `main`\n",
    "- drop `nation` table\n",
    "- add data to `region` table\n",
    "- compare `experiment` and `main` tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie branch experiment main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the default branch\n",
    "hadoop_conf.set('nessie.ref', 'experiment')\n",
    "\n",
    "catalog = jvm.NessieCatalog(hadoop_conf)\n",
    "catalog.dropTable(jvm.TableIdentifier.parse(\"testing.nation\"), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"set spark.hadoop.nessie.ref=experiment\")\n",
    "spark.sql('INSERT INTO TABLE nessie.testing.region VALUES (5, \"AUSTRALIA\", \"Let\\'s hop there\", \"AUS\")')\n",
    "spark.sql('INSERT INTO TABLE nessie.testing.region VALUES (6, \"ANTARTICA\", \"It\\'s cold\", \"ANT\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nessie contents --list --ref experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at the contents of the region table on the experiment branch. Notice the use of the `nessie` catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.region\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and compare to the contents of the region table on the main branch. Notice the use of `@main` to view data on the main branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from nessie.testing.`region@main`\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
